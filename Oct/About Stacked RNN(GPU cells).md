# About Stacked RNN(GPU cells)

> [산업제어시스템 보안위협 탐지 AI 경진대회](https://dacon.io/competitions/official/235624/codeshare/1458?page=1&dtype=recent&ptype=pub)의 `Baseline code`에서 쓰였던 `model` **Stacked RNN**, **Bidirectional GRU**에 대해 알아본다. 



## 1. 시계열 데이터에 대하여

![image](https://user-images.githubusercontent.com/58945760/91179349-5721ee80-e721-11ea-94ba-a91ef5cfcbe5.png)

![image](https://user-images.githubusercontent.com/58945760/91179321-50937700-e721-11ea-941b-d530c2653266.png)

△ 대표적인 시계열 데이터인 **환율**과 **주가**



 RNN은 시계열 데이터를 다루는 데 주로 사용되는 모델이다. 그렇다면 우선 시계열 데이터가 뭔지부터 알아봐야 한다. 

위키백과에 따르면, 시계열은 일정 시간 간격으로 배치된 데이터의 수열을 뜻한다. 시계열에 대해 많은 설명이 웹 상에 존재하지만 그나마 이 설명이 제일 간단해 보였다. 그런데도 어렵다.  좀 더 쉬운 설명은 없을까. **시간, 혹은 시간에 따른 순서에 영향을 받는 거의 모든 데이터**라고 보는 게 그나마 가장 쉬울 듯하다. 

사실상 많은 수의 데이터들은 시간이라는 요소에서 큰 영향을 받는다. 당장 평소 말할 때 쓰는 문장(실제로 언어를 다루는 자연어(aka NLP)는 RNN이 주도하는 분야이다)이 그렇고, 스트리밍으로 듣는 음악이 그렇다. 시간은 단순한 숫자가 아니라 그 숫자가 상징하는 어떠한 기간 안에 일어난 모든 사건들과 그 영향을 포괄하는 개념이 될 수 있다. 때문에 어떠한 데이터라도 시계열에 포함되는 특성을 가지고 있다는 생각도 든다. 

또한 이러한 시간에 따른 데이터의 변화는 지속적이다. 데이터는 끊임없이 시간과 상호작용하여 변화한다. 그러나 그 변화는 특정한 패턴이 존재한다는 것이 핵심이다. 과거의 데이터들이 변화하는 과정에서의 패턴을 알아내고,  현재, 미래의 데이터에 예측할 수 있도록 하는 것이 시계열 데이터 분석의 가장 중요한 부분이라고 말할 수 있다. 그렇다면 이런 데이터를 다루는 RNN은 어떤 구조, 어떤 특징을 가지고 있을까?



## 2. RNN(Recurrent Neural Network, 순환신경망)

![image](https://user-images.githubusercontent.com/58945760/91629511-a5c2d780-ea04-11ea-9fbb-2ee78b28c788.png)

<center>▲ RNN의 기본 구조



 Recurrent Neural Network, 통칭 RNN의 가장 중요한 부분은 사실 그 이름에서부터 드러난다.  Recurrent는 **반복**이라는 뜻을 가지고 있으며, 이는 한국어 명칭인 순환신경망의 **순환**과도 연결되는 부분이다.  일반적인 신경망 모델은 input부터 output까지, 한 방향으로만 직진한다. 하지만 왜 RNN은 다른가? 복잡한 수식은 잠시 제쳐두고, 초등학교 때 풀었던 빈칸 넣기 문제를 생각해보자. 



문제 1. 다음의 빈칸에 들어갈 단어를 고르시오.

____

*오늘 점심에 □를 먹었다*

____



Ⅰ. 피자

Ⅱ. 자동차

Ⅲ. 책

Ⅳ. 설렁탕



보자마자 1번이 답이란 걸 알 수 있다. 하지만 답을 맞혔다고 좋아하기 전에, 어떻게 피자라는 답을 낼 수 있었는지를 떠올려보자. 우선은 문장 끝부분의 '먹었다'는 동사다. 해당 동사는 밥, 빵과 같이 먹을 수 있는 것 뒤에만 붙는다. 그 다음은 빈칸 바로 뒤에 있는 '를'이라는 조사다. 통상 한국어에서 이런 조사는 받침이 없는 명사 뒤에만 붙을 수 있다. 따라서 답은 1이 된다.

보다시피 이 문제를 풀기 위해서는 빈칸의 앞부분뿐만 아니라 문장 전체를 모두 읽고 이해해야 함을 알 수 있다. 읽는다는 것은 두 가지 뜻을 포함한다. 첫 번째는 '문장을 기억한다'는 것이고 두 번째는 일방적이 아닌 양방향적으로 '정보를 주고받는다'는 것이다. RNN은 그 중 후자보다는 전자에 초점을 맞추었다고 볼 수 있다. 



![image](https://user-images.githubusercontent.com/58945760/91630342-81b6c480-ea0b-11ea-8ab8-1a85935b99b1.png)



위의 그림에서 뉴런으로 돌아가는 화살표들을 볼 수 있는데, 이것을 Recurrent Weight라고 부르며 과거의 데이터 정보를 현재 시점으로 전달해주는 역할을 해준다. 과거의 과거, 가장 처음 학습했던 데이터에 대한 정보까지도 전해주기 때문에 마치 위의 문장을 읽고, '기억하는' 것처럼 학습하는 게 가능하다. 

지금까지의 내용만 보면 뭔가 굉장하다. 과연 인공신경망의 꽃이라고 불릴 만하다라는 생각이 스물스물 들기 시작하지만, 세상에 완벽한 것은 존재하지 않듯이 RNN에도 치명적인 약점이 존재한다. 그러나 그 문제에 대해서는 조금 뒤로 미루도록 하겠다. 



## 3. Stacked RNN

> 여러 개의 RNN 층을 쌓아 만든 다중 RNN 모델 

![img](https://lh6.googleusercontent.com/rC1DSgjlmobtRxMPFi14hkMdDqSkEkuOX7EW_QrLFSymjasIM95Za2Wf-VwSC1Tq1sjJlOPLJ92q7PTKJh2hjBoXQawM6MQC27east67GFDklTalljlt0cFLZnPMdhp8erzO)



이제 조금은 RNN에 대해서 알 것 같다. 그렇다면 Stacked RNN은 무엇일까? 또다시 영단어를 찾아본다. 'Stacked'는 '쌓인'이라는 뜻을 가지고 있다. DNN과 CNN을 배워본 사람의 귀가 쫑긋한다. Stacked RNN이란 마치 DNN에서 은닉층을 여러 개 쌓아 모델을 만들듯이,  RNN 레이어를 여러 개 쌓아 만든 모델을 뜻한다. 

인용에 따르면 정확도로 비교했을 때는 통상적인 RNN모델과 그리 큰 차이가 나지 않는다고 한다. 



## 4. Bidirectional GRU

### 4.1 Bidirectional RNN 

> 양방향으로 미래를 예측하는 RNN 모델

![image](https://user-images.githubusercontent.com/58945760/91629733-76ad6580-ea06-11ea-9157-a227e70f99bf.png)

그렇다면 이번엔 bidirectional GRU에 대해 알아보자. 위에서는 RNN이라도 익숙했는데 이번엔 당최 무슨 말인지 모르겠다. 하나하나씩 풀어 보자. 우선 bidirectional이란 '양방향적인'이라는 뜻이다. 왠지 익숙한 말이라면 맞다. 빈칸 넣기 문제를 예로 들면서 양방향적인 정보 전달이 필요하다는 말을 했었다. bidirectional RNN 모델은 바로 이러한 생각에서 착안되었다. 

위의 그림에서처럼 이 모델은 기초적인 RNN 모델처럼 과거의 데이터를 전달받는 동시에, 거꾸로 가장 미래에 있는 데이터로부터도 정보를 전달받는다. 기존의 RNN이 가지고 있던 '과거의 데이터를 기억, 전달하는 것'에 '양방향적 정보 전달'까지 더해진 모델이라 볼 수 있다. 위의 예시에서 먹었다와 를이라는 미래의 동/조사로 빈칸을 유추해냈듯이 말이다. 양쪽으로 오는 정보를 모두 기억해야 하기에 이 모델은 메모리를 2배 잡아먹는다고 한다. 아래의 식을 보면, 서로 다른 방향에서 얻은 정보들이 마지막 식에서 합쳐지는 것을 볼 수 있다.  

![image](https://user-images.githubusercontent.com/58945760/91630931-a7929800-ea10-11ea-88d9-1b7fa0777c64.png)

위의 다중 RNN 모델처럼 층을 늘려 사용하는 것도 가능하다.



### 4.2 GRU(Gate Recurrent Unit)

그렇다면 GRU는?  GRU는 간단히 말하자면 RNN을 변형시킨 구조 중 하나다. 그 유명한 LSTM도 RNN을 변형시켜 더욱 발전시킨 구조이다. 

더 구체적으로 설명하기 전에, RNN의 개념에 대해 설명했던 앞부분으로 다시 돌아가 보자. RNN은 분명 대단한 모델이지만, 그럼에도 치명적인 약점이 존재한다고 이야기했었다. 왜 갑자기 딴소리냐고? 생각해보자. 뭔가 발전시키려는 노력이 있었다면 기존 구조에 뭔가 부족한 점, 약점이라 부를 만한 단점을 해결하기 위해서가 아니겠는가?  그렇다면 그 약점은 무엇이었을까? 



####  장기 의존성 문제(the problem of Long-Term Dependencies )

![image](https://user-images.githubusercontent.com/58945760/91631876-ebd56680-ea17-11ea-9118-6d7e69f67fc2.png)

<center>▲ 만악의 근원 사라지는 경사 문제(Vanishing gradient) 시각화



*"1년 전 내 새해 목표는 금연이었고, 5년 전 목표는 애인 사귀는 거였고, 10년 전 목표는......어라....? 그 전에는 뭐였더라? "*



위에서 RNN은 마치 우리가 평소 다른 사람과 대화할 때 주고받은 말들을 기억하듯이, 과거의 정보를 계속 전달하는 것이 특징이라고 말했었다. 하지만 상황을 바꿔 생각해보자. 만약 대학의 친구와 짧게 주고받은 수다가 아니라, 졸업식에서 학장의 10분짜리 연설을 듣는 것이었다면? 무교인데 친척에게 끌려와 앉은 교회에서 목사님의 말씀을 30분째 듣고만 있을 때라면? 목사님과 학장님이 처음에 무슨 말을 했는지 우리가 기억할 수 있을까? 당연하지만 기억날 리가 없다. 몇 분 전, 아니 몇 초 전에 한 말이면 몰라도, 한참 전에 했던 말을 어떻게 기억하겠는가? 불행히도 대부분의 사람은 완전기억능력이라는 축복을 누리지 못한다. 

RNN에서도 이와 비슷한 문제가 발생한다. 간단한, 단순한 데이터라면 문제가 되지 않는다. 이를테면 Hi Minsu 같은 거. 하지만 문단이라면? 기사 한 면이라면? 장편 소설이라면? 전달해야 할 과거의 정보가 기하급수적으로 늘어나는 와중에도 새로운 정보는 계속 들어온다. 새로운 정보를 중시하다 보면, 과거의 정보, 그보다 훨씬 과거의 정보가 계산에 미치는 영향은 빠르게 줄어들게 된다. 최초의 데이터는 어느 순간 의미가 없어지고, 그 말은 곧 RNN이 가지는 특성이자 장점이 사라진다는 뜻이 된다. 

이러한 현상의 구체적인 원인은 DNN에서도 골치였던 사라지는/폭발하는 경사 문제(Vanishing/exploding gradient) 때문으로, 최적화를 위해 사용된 활성화 함수를 거친 값이 0으로 수렴하거나,  미친듯이 커지는 바람에 제대로 된 예측이나 분류를 할 수 없는 상황이 되는 것이다. 이 문제에 대해서는 이 정도만 언급해 두겠습니다. 중요한 것은 과거로부터 전달되는 정보가 소실되어버리는 점이 RNN이란 모델에 있어 무엇보다도 치명적인 약점으로 여겨졌고, 그를 해결하기 위해 나온 것이 LSTM이며, 그리고 우리가 알아볼 GPU도 그 중 하나라는 점이다. 



#### LSTM(Long Short-Term Memory)

> 3개의 Gate(forget, input, output)를 이용하여 정보 손실을 막고 효율적인 정보 전달

![image](https://user-images.githubusercontent.com/58945760/91632809-4ec9fc00-ea1e-11ea-9d28-d6fc734dade2.png)

LSTM은 과거의 정보를 얼마나 잊을지, 현재의 정보를 얼마나 기억할지를 Gate를 통해 조절한다. 또한 이렇게 취합된 정보에 한 번 더 활성화 함수를 적용, 전달할 정보를 효율화하고, 정보의 손실이 일어나는 것을 방지한다.  



#### GPU

> 2개의 Gate(reset, update)로 전달할 정보 효율화, LSTM을 보다 간략화한 구조

![image](https://user-images.githubusercontent.com/58945760/91632822-64d7bc80-ea1e-11ea-8868-e4038f892523.png)

  GPU의 경우 LSTM의 그림과 비교했을 때 느껴지듯이 Gate가 하나 더 적지만 원리는 비슷하다. 그렇기에 RNN을 변형했다기보다는 LSTM을 변형한 구조라고 말하는 것이 더 맞을지도 모른다. reset gate는 새로운 입력 정보를 이전 정보와 어떻게 합칠지 정하고, update gate의 경우 과거로부터 받은 정보를 얼마나 기억하고 남길지를 정한다.

LSTM보다 간단한 구조임에도 성능은 큰 차이가 없기 때문에, 최근 들어 많은 관심을 받고 있기도 하다.



### 돌아가서

따라서 Bidirectional GRU는, 양방향적으로 정보를 전달하여 필요한 값을 예측하는 동시에, GRU의 구조를 가진 레이어들로 정보 전달의 효율성을 최대화하도록 구성한 모델이라 할 수 있다.  



참고 : https://m.blog.naver.com/ekbae98/221265719881

https://dalpo0814.tistory.com/36

https://blog.naver.com/2011topcit/220610525815

https://buomsoo-kim.github.io/keras/2019/07/12/Easy-deep-learning-with-Keras-19.md/

https://wikidocs.net/22889

https://gruuuuu.github.io/machine-learning/lstm-doc2/#